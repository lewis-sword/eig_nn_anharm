{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9627ff-76b3-4b1d-819d-5e0c50cda17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from scipy.integrate import odeint\n",
    "dtype=torch.float\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261f5f46-253b-4a63-8b72-9a98382e8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the plots\n",
    "plt.rc('xtick', labelsize=16) \n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Define the sin() activation function\n",
    "class mySin(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(input):\n",
    "        return torch.sin(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2be70a5-93c4-48f9-932a-5373814846f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some more general functions\n",
    "def dfx(x,f):\n",
    "    \"\"\"Calculate the derivative with auto-differention\"\"\"\n",
    "    return grad([f], [x], grad_outputs=torch.ones(x.shape, dtype=dtype), create_graph=True)[0]\n",
    "\n",
    "def perturbPoints(grid,t0,tf,sig=0.5):\n",
    "    \"\"\"stochastic perturbation of the evaluation points force t[0]=t0  & force points to be in the t-interval\"\"\"\n",
    "    delta_t = grid[1] - grid[0]  \n",
    "    noise = delta_t * torch.randn_like(grid)*sig\n",
    "    t = grid + noise\n",
    "    t.data[2] = torch.ones(1,1)*(-1)\n",
    "    t.data[ttf]=2*tf - t.data[t>tf]\n",
    "    t.data[0] = torch.ones(1,1)*t0\n",
    "\n",
    "    t.data[-1] = torch.ones(1,1)*tf\n",
    "    t.requires_grad = False\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94736c3b-3fab-4c95-ba63-ecdaae26b6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 6.], grad_fn=<MulBackward0>)\n",
      "tensor([3., 3.])\n"
     ]
    }
   ],
   "source": [
    "#Testing gradients with autograd\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = a**2+b*3\n",
    "#external_grad = torch.tensor([1., 1.])\n",
    "#Q.backward(gradient=external_grad)\n",
    "#grad(x,torch.sin(x))\n",
    "#b.grad\n",
    "#Cannot call backward before this.\n",
    "#grad([Q],[a], grad_outputs=torch.ones(a.shape, dtype=dtype), retain_graph=True)\n",
    "print(dfx(a,Q))\n",
    "print(dfx(b,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ab26ab7-b70b-4a27-90c2-840d1fe8a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qNN1(torch.nn.Module):\n",
    "    def __init__(self, D_hid=10):\n",
    "        super(qNN1,self).__init__()\n",
    "\n",
    "        # Define the Activation\n",
    "        #  self.actF = torch.nn.Sigmoid()   \n",
    "        self.actF = mySin()\n",
    "        \n",
    "        # define layers\n",
    "        #self.Lin_1   = torch.nn.Linear(1, D_hid)\n",
    "        #self.E_out = torch.nn.Linear(D_hid, 1)\n",
    "        #self.Lin_2 = torch.nn.Linear(D_hid, D_hid)\n",
    "        #self.Ein = torch.nn.Linear(1,1)\n",
    "        #self.Lin_out = torch.nn.Linear(D_hid+1, 1)\n",
    "        \n",
    "        self.Ein    = torch.nn.Linear(1,1)\n",
    "        self.Lin_1  = torch.nn.Linear(2, D_hid)\n",
    "        self.Lin_2  = torch.nn.Linear(D_hid, D_hid)\n",
    "        self.out    = torch.nn.Linear(D_hid, 1)\n",
    "\n",
    "    def forward(self,t):\n",
    "        In1 = self.Ein(torch.ones_like(t))\n",
    "        L1 = self.Lin_1(torch.cat((t,In1),1))\n",
    "        h1 = self.actF(L1)\n",
    "        L2 = self.Lin_2(h1)\n",
    "        h2 = self.actF(L2)\n",
    "        out = self.out(h2)\n",
    "        return out, In1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5be562a-a7ca-4fcb-9620-3d112b9bb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def potential(Xs):\n",
    "  # Gives the potential at each point\n",
    "  # Takes in tensor of x points, gives back tensor of V at each point\n",
    "  k0 = 2\n",
    "  g = 1\n",
    "\n",
    "\n",
    "  Xsnp = Xs.data.numpy()\n",
    "  Vnp = k0*Xsnp**2/2 + g*Xsnp**4\n",
    "  Vtorch = torch.from_numpy(Vnp)\n",
    "  return Vtorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c61aaa53-fa1c-4f9b-ad76-3a3fd3a971af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN\n",
    "def run_Scan_oscillator(t0, tf, x1, neurons, epochs, n_train,lr, minibatch_number = 1):\n",
    "    fc0 = qNN1(neurons)\n",
    "    fc1=0; \n",
    "    betas = [0.999, 0.9999]\n",
    "    optimizer = optim.Adam(fc0.parameters(), lr=lr, betas=betas)\n",
    "    Loss_history = [];     Llim =  1e+20\n",
    "    En_loss_history = []\n",
    "    boundary_loss_history = []\n",
    "    nontriv_loss_history = []\n",
    "    SE_loss_history = []\n",
    "    Ennontriv_loss_history = []\n",
    "    criteria_loss_history = []\n",
    "    En_history = []\n",
    "    EWall_history = []\n",
    "    di = (None, 1e+20)\n",
    "    dic = {0:di, 1:di, 2:di, 3:di, 4:di, 5:di , 6:di, 7:di, 8:di, 9:di, 10:di, 11:di, 12:di, 13:di, 14:di, 15:di, 16:di}\n",
    "    \n",
    "    grid = torch.linspace(t0, tf, n_train).reshape(-1,1)\n",
    "    \n",
    "    ## TRAINING ITERATION    \n",
    "    TeP0 = time.time()\n",
    "    walle = -2\n",
    "    last_psi_L = 0\n",
    "    for tt in range(epochs): \n",
    "        #adjusting learning rate at epoch 3e4\n",
    "        #if tt == 3e4:\n",
    "        #    optimizer = optim.Adam(fc0.parameters(), lr = 1e-2, betas = betas)\n",
    "# Perturbing the evaluation points & forcing t[0]=t0\n",
    "        t=perturbPoints(grid,t0,tf,sig=.03*tf)\n",
    "            \n",
    "# BATCHING\n",
    "        batch_size = int(n_train/minibatch_number)\n",
    "        batch_start, batch_end = 0, batch_size\n",
    "\n",
    "        idx = np.random.permutation(n_train)\n",
    "        t_b = t[idx]\n",
    "        t_b.requires_grad = True\n",
    "        t_f=t[-1]\n",
    "        t_f=t_f.reshape(-1,1)\n",
    "        t_f.requires_grad = True\n",
    "        loss=0.0\n",
    "\n",
    "\n",
    "        for nbatch in range(minibatch_number): \n",
    "# batch time set\n",
    "            t_mb = t_b[batch_start:batch_end]\n",
    "\n",
    "#  Network solutions \n",
    "            nn, En = fc0(t_mb)\n",
    "\n",
    "            En_history.append(En[0].data.tolist()[0])\n",
    "\n",
    "            psi  = parametricSolutions(t_mb, fc0, t0, x1) #- last_psi_L*torch.exp(-(torch.ones_like(t_mb)-1)**2/(2*(1/20)))\n",
    "            #last_psi_L = parametricSolutions(torch.ones_like(t_mb),fc0,t0,x1).data.numpy()[0][0]\n",
    "            #print(last_psi_L)\n",
    "            Pot = potential(t_mb)\n",
    "            Ltot = hamEqs_Loss(t_mb, psi, En, Pot)\n",
    "            SE_loss_history.append(Ltot) #\n",
    "            \n",
    "            criteria_loss =  Ltot\n",
    "\n",
    "            if tt%1000 == 0:\n",
    "              walle += 0.16\n",
    "            Ltot += 1/((psi.pow(2)).mean()+1e-6) + 1/(En.pow(2).mean()+1e-6) + torch.exp(-1*En+walle).mean()\n",
    "            En_loss_history.append(torch.exp(-1*En+walle).mean()) #\n",
    "            EWall_history.append(walle)\n",
    "\n",
    "            \n",
    "            nontriv_loss_history.append(1/((psi.pow(2)).mean()+1e-6)) #\n",
    "            Ennontriv_loss_history.append(1/(En.pow(2).mean()+1e-6)) #\n",
    "# OPTIMIZER\n",
    "            Ltot.backward(retain_graph=False); #True\n",
    "            optimizer.step(); loss += Ltot.data.numpy()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_start +=batch_size\n",
    "            batch_end +=batch_size\n",
    "\n",
    "# keep the loss function history\n",
    "        Loss_history.append(loss)       \n",
    "\n",
    "#Keep the best model (lowest loss) by using a deep copy\n",
    "        if  criteria_loss < Llim:\n",
    "            fc1 =  copy.deepcopy(fc0)\n",
    "            Llim=criteria_loss\n",
    "\n",
    "        E_bin = abs(En[0].data.tolist()[0]//2) \n",
    "        if criteria_loss < dic[E_bin][1]:\n",
    "          dic[E_bin] = (copy.deepcopy(fc0), criteria_loss)\n",
    "\n",
    "    TePf = time.time()\n",
    "    runTime = TePf - TeP0  \n",
    "    loss_histories = (Loss_history, boundary_loss_history, nontriv_loss_history, SE_loss_history, Ennontriv_loss_history, En_loss_history, criteria_loss_history, fc0, En_history, EWall_history, dic)\n",
    "    return fc1, loss_histories, runTime\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4980ca0-17a3-403d-a843-75a97b237584",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m xBC1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m      7\u001b[0m n_train, neurons, epochs, lr,mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1200\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m5e4\u001b[39m), \u001b[38;5;241m8e-3\u001b[39m, \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m----> 8\u001b[0m model1,loss_hists1,runTime1 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_Scan_oscillator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxBC1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneurons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m, in \u001b[0;36mrun_Scan_oscillator\u001b[1;34m(t0, tf, x1, neurons, epochs, n_train, lr, minibatch_number)\u001b[0m\n\u001b[0;32m     24\u001b[0m     last_psi_L \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs): \n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m#adjusting learning rate at epoch 3e4\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m#if tt == 3e4:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m#    optimizer = optim.Adam(fc0.parameters(), lr = 1e-2, betas = betas)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Perturbing the evaluation points & forcing t[0]=t0\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m         t\u001b[38;5;241m=\u001b[39m\u001b[43mperturbPoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43msig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.03\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# BATCHING\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train\u001b[38;5;241m/\u001b[39mminibatch_number)\n",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m, in \u001b[0;36mperturbPoints\u001b[1;34m(grid, t0, tf, sig)\u001b[0m\n\u001b[0;32m     11\u001b[0m t \u001b[38;5;241m=\u001b[39m grid \u001b[38;5;241m+\u001b[39m noise\n\u001b[0;32m     12\u001b[0m t\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mtf \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mdata[t\u001b[38;5;241m>\u001b[39mtf]\n\u001b[0;32m     14\u001b[0m t\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mt0\n\u001b[0;32m     16\u001b[0m t\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mtf\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [0]"
     ]
    }
   ],
   "source": [
    "\n",
    "## Train the model \n",
    "\n",
    "t0 = -6\n",
    "tf = 6\n",
    "xBC1=0.\n",
    "\n",
    "n_train, neurons, epochs, lr,mb = 1200, 50, int(5e4), 8e-3, 1 \n",
    "model1,loss_hists1,runTime1 = run_Scan_oscillator(t0, tf, xBC1, neurons, epochs, n_train, lr, mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b62cd1-b467-46a5-ab85-23b66427b52d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'runTime1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loss function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining time (minutes):\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mrunTime1\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mloglog(loss_hists1[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-b\u001b[39m\u001b[38;5;124m'\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.975\u001b[39m);\n",
      "\u001b[1;31mNameError\u001b[0m: name 'runTime1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss function\n",
    "print('Training time (minutes):', runTime1/60)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.loglog(loss_hists1[0],'-b',alpha=0.975);\n",
    "#plt.axvline(x = aarg)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Total Loss');plt.xlabel('Epochs')\n",
    "#plt.savefig(imgdir+'harmonic_total_loss.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42b46204-c6b1-4176-9574-46a2373bf26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m tTest\u001b[38;5;241m.\u001b[39mrequires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m t_net \u001b[38;5;241m=\u001b[39m tTest\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m----> 6\u001b[0m psi \u001b[38;5;241m=\u001b[39mparametricSolutions(tTest,\u001b[43mmodel1\u001b[49m,t0,xBC1) \n\u001b[0;32m      7\u001b[0m psi\u001b[38;5;241m=\u001b[39mpsi\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(); \n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "# TEST THE PREDICTED SOLUTIONS\n",
    "nTest = n_train; tTest = torch.linspace(t0-.1,tf+.1,nTest)\n",
    "tTest = tTest.reshape(-1,1);\n",
    "tTest.requires_grad=True\n",
    "t_net = tTest.detach().numpy()\n",
    "psi =parametricSolutions(tTest,model1,t0,xBC1) \n",
    "psi=psi.data.numpy(); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a572202e-c5fd-4392-b978-29f200716feb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 9) (2997111374.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.ylabel('\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#tru = np.sin(3*np.pi*t_net)*np.max(-1*psi)\n",
    "#plt.plot(t_net, tru, '-r', linewidth = 1, label = 'True')\n",
    "plt.xlim(-7,7)\n",
    "#\n",
    "plt.plot(t_net, 1*psi, '-b', linewidth=1, label = 'ANN')\n",
    "plt.legend()\n",
    "plt.plot(t_net, np.zeros(len(t_net)),'--k', linewidth=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('\n",
    "')\n",
    "plt.grid('on')\n",
    "#plt.plot(t_net, 8*t_net**2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb370a-df32-4c83-bba1-f1d70d05dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(loss_hists1[8])\n",
    "#plt.axvline(x = aarg)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Model Energy History');plt.xlabel('Epochs')\n",
    "plt.savefig(imgdir+'harmonic_modelE_hist.png', bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
